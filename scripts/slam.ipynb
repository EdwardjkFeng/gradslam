{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynmaic $\\nabla$ SLAM\n",
    "This script is mainly for testing on static scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import gradslam related modules\n",
    "# import gradslam as gs\n",
    "from gradslam import Pointclouds, RGBDImages\n",
    "from gradslam.datasets import Cofusion, ICL, TUM\n",
    "from gradslam.slam import PointFusion, ICPSLAM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import open3d as o3d\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "torch.cuda.empty_cache() # clean cuda cache"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Supported datasets are `TUM-RGBD`, `ICL-NUIM` and `CoFusion`. Among them, only `CoFusion` sequences contain prior semantic segmentation masks for dynamic objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 115 #212\n",
    "\n",
    "data_set = 'CoFusion'\n",
    "data_path = '/home/jingkun/Dataset/'\n",
    "load_masks = False\n",
    "\n",
    "match data_set:\n",
    "    case 'CoFusion':\n",
    "        cofusion_path = data_path + 'CoFusion/'\n",
    "\n",
    "        sequences = (\"room4-full\",) # 850 frames\n",
    "\n",
    "        load_masks = True\n",
    "\n",
    "        # Load data\n",
    "        dataset = Cofusion(basedir=cofusion_path, sequences=sequences, seqlen=n_frames, dilation=0, start=540, height=240, width=320, channels_first=False, return_object_mask=load_masks)\n",
    "    case 'ICL':\n",
    "        icl_path = data_path + 'ICL/' # associated 880 frames\n",
    "\n",
    "        # load dataset\n",
    "        dataset = ICL(icl_path, trajectories=(\"living_room_traj2_frei_png\",) ,seqlen=n_frames, dilation=1, start=200, height=240, width=320, channels_first=False)\n",
    "\n",
    "    case 'TUM':\n",
    "        tum_path = data_path + 'TUM/'\n",
    "        sequences = (\"rgbd_dataset_freiburg3_cabinet\",) # associated 792 frames\n",
    "\n",
    "        # Load data\n",
    "        dataset = TUM(basedir= tum_path, sequences=sequences, seqlen=n_frames, dilation=3, height=240, width=320)\n",
    "\n",
    "loader = DataLoader(dataset=dataset, batch_size=1)\n",
    "if load_masks:\n",
    "    colors, depths, intrinsics, poses, *_, names, masks, labels = next(iter(loader))\n",
    "else:\n",
    "    colors, depths, intrinsics, poses, *_, names = next(iter(loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"colors shape: {colors.shape}\")  # torch.Size([2, 8, 240, 320, 3])\n",
    "print(f\"depths shape: {depths.shape}\")  # torch.Size([2, 8, 240, 320, 1])\n",
    "print(f\"intrinsics shape: {intrinsics.shape}\")  # torch.Size([2, 1, 4, 4])\n",
    "print(f\"poses shape: {poses.shape}\")  # torch.Size([2, 8, 4, 4])\n",
    "print(f\"masks shape: {masks.shape}\") if load_masks else None# torch.Size([1, 200, 480, 640, 3])\n",
    "print(f\"labels shape: {labels.shape}\") if load_masks else None\n",
    "print('---')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize RGB and Depth Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RGB-D image objects \n",
    "rgbdimages = RGBDImages(colors.requires_grad_(False),\n",
    "                        depths.requires_grad_(False), \n",
    "                        intrinsics.requires_grad_(False),\n",
    "                        poses.requires_grad_(False),\n",
    "                        channels_first=False,\n",
    "                        object_mask=masks,\n",
    "                        object_label=labels,\n",
    "                        filter_objects=True\n",
    "                        )\n",
    "\n",
    "print(rgbdimages.shape)\n",
    "\n",
    "rgbdimages.plotly(0).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up CUDA Caches\n",
    "The loaded data occupy the large portion of the GPU memory, which may have a bad impact on the speed of the SLAM system. Since they are parsed in `RGBDImages` variables and no longer needed for the following steps, we can delete them and clean up the CUDA cache to free the occupied GPU storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loader, colors, depths, poses\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gpu_usage()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLAM\n",
    "### Basic SLAM\n",
    "Run the SLAM on the loaded sequences, output the estimated camera poses of every frame and visualize the constructed map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device \", device)\n",
    "slam = PointFusion(odom='dia', numiters=30, dsratio=3, device=device).requires_grad_(False)\n",
    "\n",
    "print(rgbdimages.channels_first)\n",
    "pcds, transformation = slam.forward(rgbdimages.to(device))\n",
    "o3d.visualization.draw_geometries([pcds.open3d(0, max_num_points=n_frames*100000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer `poses` from GPU to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = transformation.cpu().detach().squeeze().numpy()\n",
    "print(poses.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLAM Step-by-step\n",
    "Feed frames in system one after one and visualize the temoperal aggregated map.\n",
    "\n",
    "(To note that, we use `open3D`-visualizer for the visualization purpose in such way that after a new frame is processed, the newly local reconstruction (point cloud) is append to the visualizer. However, in our implementation, dupplicated points would not be pruned. Hence, the process time of visualization is proprotional to the number of fed frames.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_TRANSFORM = False\n",
    "INTRINSIC = intrinsics.squeeze().numpy()[:3, :3]\n",
    "INT_POSE = np.eye(4)\n",
    "# Fuse point clouds\n",
    "list_t_frame = []\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device \", device)\n",
    "slam = PointFusion(odom='icp', numiters=10, dsratio=4, device=device).requires_grad_(False)\n",
    "\n",
    "diaslam = PointFusion(odom='dia', numiters=30, dsratio=3, device=device)\n",
    "\n",
    "pcds = Pointclouds(device=device)\n",
    "batch_size, seq_len = rgbdimages.shape[:2]\n",
    "initial_poses = torch.eye(4, device=device).view(1, 1, 4, 4).expand(batch_size, -1, -1, -1)\n",
    "prev_frame = None\n",
    "poses = []\n",
    "\n",
    "# Create visualizer and window.\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(height=1000, width=1000)\n",
    "intermediate_pcd = o3d.geometry.PointCloud()\n",
    "vis.add_geometry(intermediate_pcd)\n",
    "bouding_box = o3d.geometry.OrientedBoundingBox()\n",
    "R_y_180 = np.eye(4, dtype=float)\n",
    "R_y_180[1, 1] = R_y_180[2, 2] = -1.0\n",
    "R_z_180 = np.eye(4, dtype=float)\n",
    "R_z_180[0, 0] = R_z_180[1, 1] = -1\n",
    "\n",
    "\n",
    "# Initialize coordinate system and fustum in open3D\n",
    "world_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05) #.transform(R_z_180)\n",
    "fustum_int = o3d.geometry.LineSet.create_camera_visualization(320, 240, INTRINSIC, INT_POSE, 0.07)\n",
    "\n",
    "start_ = time.time()\n",
    "s = 0\n",
    "keep_running = True\n",
    "# for s in range(seq_len):\n",
    "local_pcds = Pointclouds(device=device)\n",
    "\n",
    "while keep_running:\n",
    "    if s < seq_len:\n",
    "        vis.remove_geometry(intermediate_pcd)\n",
    "        if bouding_box is not None:\n",
    "            vis.remove_geometry(bouding_box)\n",
    "        del intermediate_pcd\n",
    "\n",
    "        start = time.time()\n",
    "        live_frame = rgbdimages[:, s].to(device)\n",
    "        if s == 0 and live_frame.poses is None:\n",
    "            live_frame.poses = initial_poses\n",
    "\n",
    "        pcds, live_frame.poses, live_frame.init_T = diaslam.step(pcds, live_frame, prev_frame, inplace=False)\n",
    "\n",
    "        # bb_corners = get_bouding_box_via_PCA(pcds)\n",
    "\n",
    "        intermediate_pcd = pcds.open3d(0, max_num_points=n_frames*5000)\n",
    "        intermediate_pcd.transform(R_y_180)\n",
    "\n",
    "        # bouding_box = o3d.geometry.Geometry3D.get_oriented_bounding_box(intermediate_pcd)\n",
    "        # bouding_box.color = [1, 0, 0]\n",
    "        # bouding_box = o3d.geometry.Geometry3D.get_oriented_bounding_box(o3d.utility.Vector3dVector(bb_corners.cpu().detach().squeeze().numpy()))\n",
    "        # bouding_box.color = [1, 0, 0]\n",
    "        # bouding_box.transform(R_y_180)\n",
    "\n",
    "        vis.add_geometry(intermediate_pcd)\n",
    "        # vis.add_geometry(bouding_box)\n",
    "        # Transfer pose to numpy in cpu \n",
    "        pose = live_frame.poses.cpu().detach().squeeze().numpy()\n",
    "        camera_pose = copy.deepcopy(world_frame).transform(pose).transform(R_y_180)#.transform(R_z_180)\n",
    "        # fustum = o3d.geometry.LineSet.create_camera_visualization(320, 240, INTRINSIC, np.linalg.inv(pose), 0.07).transform(R_y_180)\n",
    "        fustum = copy.deepcopy(fustum_int).transform(pose).transform(R_y_180)\n",
    "        vis.add_geometry(camera_pose)\n",
    "        vis.add_geometry(fustum)\n",
    "\n",
    "        poses.append(pose)\n",
    "        \n",
    "        prev_frame = live_frame if diaslam.odom != 'gt' else None\n",
    "\n",
    "        list_t_frame.append((time.time() - start))\n",
    "        if PRINT_TRANSFORM:\n",
    "            print(\"Frame: %d, Time: %.3f\" % (s, list_t_frame[-1]))\n",
    "            print(\"Estimated pose: \", poses[s])\n",
    "\n",
    "        del live_frame\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    s += 1\n",
    "\n",
    "    keep_running = vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "\n",
    "print('Overall time :%.3f' % (time.time() - start_))\n",
    "# o3d.visualization.draw_geometries([pcds.open3d(0, max_num_points=n_frames*100000)])\n",
    "\n",
    "vis.destroy_window()\n",
    "\n",
    "# Plot graph of consumed time per frame\n",
    "plt.plot(list_t_frame)\n",
    "plt.ylabel('Consumed time per frame [ms]')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Visualize Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_pcd = o3d.io.read_point_cloud(\"../results/TUM/dia.pcd\")\n",
    "o3d.visualization.draw_geometries([icl_pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_pcd = o3d.io.read_point_cloud(\"/home/jingkun/Dataset/CoFusion/room4-noise.klg-export/cloud-0.ply\")\n",
    "# pcd1 = o3d.io.read_point_cloud(\"/home/jingkun/Dataset/CoFusion/room4-noise.klg-export/cloud-1.ply\")\n",
    "# pcd2 = o3d.io.read_point_cloud(\"/home/jingkun/Dataset/CoFusion/room4-noise.klg-export/cloud-2.ply\")\n",
    "# pcd3 = o3d.io.read_point_cloud(\"/home/jingkun/Dataset/CoFusion/room4-noise.klg-export/cloud-3.ply\")\n",
    "# o3d.visualization.draw_geometries([static_pcd, pcd1, pcd2, pcd3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results\n",
    "### Convert Rotation Matrix to Quaterion\n",
    "Helper Funciton for converting rotation matrix to quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_mat_to_quat(rot_mat: np.ndarray, isprecise: bool = False):\n",
    "    M = np.array(rot_mat, dtype=np.float64, copy=False)[:4, :4]\n",
    "    if isprecise:\n",
    "        q = np.empty((4, ))\n",
    "        t = np.trace(M)\n",
    "        if t > M[3, 3]:\n",
    "            q[0] = t\n",
    "            q[3] = M[1, 0] - M[0, 1]\n",
    "            q[2] = M[0, 2] - M[2, 0]\n",
    "            q[1] = M[2, 1] - M[1, 2]\n",
    "        else:\n",
    "            i, j, k = 1, 2, 3\n",
    "            if M[1, 1] > M[0, 0]:\n",
    "                i, j, k = 2, 3, 1\n",
    "            if M[2, 2] > M[i, i]:\n",
    "                i, j, k = 3, 1, 2\n",
    "            t = M[i, i] - (M[j, j] + M[k, k]) + M[3, 3]\n",
    "            q[i] = t\n",
    "            q[j] = M[i, j] + M[j, i]\n",
    "            q[k] = M[k, i] + M[i, k]\n",
    "            q[3] = M[k, j] - M[j, k]\n",
    "        q *= 0.5 / np.sqrt(t * M[3, 3])\n",
    "    else:\n",
    "        m00, m01, m02, m10, m11, m12, m20, m21, m22 = [entry for entry in M[:3, :3].reshape(-1)]\n",
    "        K = np.array([[m00-m11-m22, 0.0, 0.0, 0.0],\n",
    "                      [m01+m10, m11-m00-m22, 0.0, 0.0],\n",
    "                      [m02+m20, m12+m21, m22-m00-m11, 0.0],\n",
    "                      [m21-m12, m02-m20, m10-m01, m00+m11+m22]])\n",
    "        K /= 3.0\n",
    "        # Quaternion is eignevector of K that corresponds to largest eigenvalue\n",
    "        w, V = np.linalg.eigh(K)\n",
    "        # q = [x, y, z, w]\n",
    "        q = V[[0, 1, 2, 3], np.argmax(w)]\n",
    "    if q[3] < 0.0:\n",
    "        np.negative(q, q)\n",
    "    return q\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save estimates camera poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write a separated function to generate poses txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_qt = []\n",
    "# np.set_printoptions(suppress=True, formatter={'float_kind':'{:0.4f}'.format})\n",
    "for i in range(len(poses)):\n",
    "    pose = poses[i]\n",
    "    t = pose[:3, 3].reshape(-1)\n",
    "    rot_mat = np.identity(4)\n",
    "    rot_mat[:3, :3] = pose[:3, :3]\n",
    "    qt = rotation_mat_to_quat(rot_mat).reshape(-1)\n",
    "    poses_qt.append(np.append(t, qt).astype(np.float16))\n",
    "\n",
    "# names_list = names[0].strip().split(\", \")\n",
    "names_list = names[0].strip().split()[5::6]\n",
    "\n",
    "with open(\"../results/TUM/GradICP.txt\", \"w\") as f:\n",
    "    for i in range(len(names_list)):\n",
    "        # outstr = names_list[i][27:]\n",
    "        outstr = names_list[i]\n",
    "        # TODO \n",
    "        for j in range(len(poses_qt[i])):\n",
    "            outstr += \" \"\n",
    "            outstr += \"{:f}\".format(poses_qt[i][j])\n",
    "        # pose_str = ' '.join(map(str, poses_qt[i]))\n",
    "        # pose_str = np.savetxt(sys.stdout, poses_qt[i], newline=' ', fmt=\"%.4f\")\n",
    "        f.write(outstr + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.io.write_point_cloud(\"../results/TUM/gradicp.pcd\", pcds.open3d(0, max_num_points=None), compressed=True)\n",
    "# del pcds\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step map aggregating \n",
    "(Visalized via plotly, usable for browser, but really slow for large map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plotly_map_update_visualization(intermediate_pcs, poses, K, max_points_per_pc = 50000, ms_per_frame = 50):\n",
    "    def plotly_poses(poses, K):\n",
    "        fx = abs(K[0, 0])\n",
    "        fy = abs(K[1, 1])\n",
    "        f = (fx + fy) / 2\n",
    "        cx = K[0, 2]\n",
    "        cy = K[1, 2]\n",
    "\n",
    "        cx = cx / f\n",
    "        cy = cy / f\n",
    "        f = 1.\n",
    "\n",
    "        pos_0 = np.array([0., 0., 0.])\n",
    "        fustum_0 = np.array(\n",
    "            [\n",
    "                [-cx, -cy, f],\n",
    "                [cx, -cy, f],\n",
    "                list(pos_0),\n",
    "                [-cx, -cy, f],\n",
    "                [-cx, cy, f],\n",
    "                list(pos_0),\n",
    "                [cx, cy, f],\n",
    "                [-cx, cy, f],\n",
    "                [cx, cy, f],\n",
    "                [cx, -cy, f]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        traj = []\n",
    "        traj_fustums = []\n",
    "        for pose in poses:\n",
    "            rot = pose[:3, :3]\n",
    "            tvec = pose[:3, 3]\n",
    "\n",
    "            fustum_i = fustum_0 @ rot.T\n",
    "            fustum_i = fustum_i + tvec\n",
    "            pos_i = pos_0 + tvec\n",
    "\n",
    "            pos_i = np.round(pos_i, decimals=2)\n",
    "            fustum_i = np.round(fustum_i, decimals=2)\n",
    "\n",
    "            traj.append(pos_i)\n",
    "            traj_array = np.array(traj)\n",
    "            traj_fustum = [\n",
    "                go.Scatter3d(\n",
    "                    x=fustum_i[:, 0], y=fustum_i[:, 1], z=fustum_i[:, 2],\n",
    "                    marker=dict(\n",
    "                        size=0.1\n",
    "                    ),\n",
    "                    line=dict(color='purple', width=2)\n",
    "                ),\n",
    "                go.Scatter3d(\n",
    "                    x=pos_i[None, 0], y=pos_i[None, 1], z=pos_i[None, 2],\n",
    "                    marker=dict(size=0, color='purple')\n",
    "                ),\n",
    "                go.Scatter3d(\n",
    "                    x=traj_array[:, 0], y=traj_array[:, 1], z=traj_array[:, 2],\n",
    "                    marker=dict(size=0.1),\n",
    "                    line=dict(color ='purple', width=2)\n",
    "                )\n",
    "            ]\n",
    "            traj_fustums.append(traj_fustum)\n",
    "        return traj_fustums\n",
    "        \n",
    "    def frame_args(duration):\n",
    "        return {\n",
    "            \"frame\": {\"duration\": duration, \"redraw\": True},\n",
    "            \"mode\": \"immediate\",\n",
    "            \"fromcurrent\": True,\n",
    "            \"transistion\": {\"duration\": duration, \"easing\": \"linear\"}\n",
    "        }\n",
    "\n",
    "    # visualization\n",
    "    scatter3d_list = [pc.plotly(0, as_figure=False, max_num_points=max_points_per_pc, point_size=4) for pc in intermediate_pcs]\n",
    "    traj_frustums = plotly_poses(poses.cpu().numpy(), K.cpu().numpy())\n",
    "    data = [[*frustum, scatter3d] for frustum, scatter3d in zip(traj_frustums, scatter3d_list)]\n",
    "\n",
    "    steps = [\n",
    "        {\"args\": [[i], frame_args(0)], \"label\": i, \"method\": \"animate\"}\n",
    "        for i in range(seq_len)\n",
    "    ]\n",
    "    sliders = [\n",
    "        {\n",
    "            \"active\": 0,\n",
    "            \"yanchor\": \"top\",\n",
    "            \"xanchor\": \"left\",\n",
    "            \"currentvalue\": {\"prefix\": \"Frame: \"},\n",
    "            \"pad\": {\"b\": 10, \"t\": 60},\n",
    "            \"len\": 0.9,\n",
    "            \"x\": 0.1,\n",
    "            \"y\": 0,\n",
    "            \"steps\": steps,\n",
    "        }\n",
    "    ]\n",
    "    updatemenus = [\n",
    "        {\n",
    "            \"buttons\": [\n",
    "                {\n",
    "                    \"args\": [None, frame_args(ms_per_frame)],\n",
    "                    \"label\": \"&#9654;\",\n",
    "                    \"method\": \"animate\",\n",
    "                },\n",
    "                {\n",
    "                    \"args\": [[None], frame_args(0)],\n",
    "                    \"label\": \"&#9724;\",\n",
    "                    \"method\": \"animate\",\n",
    "                },\n",
    "            ],\n",
    "            \"direction\": \"left\",\n",
    "            \"pad\": {\"r\": 10, \"t\": 70},\n",
    "            \"showactive\": False,\n",
    "            \"type\": \"buttons\",\n",
    "            \"x\": 0.1,\n",
    "            \"xanchor\": \"right\",\n",
    "            \"y\": 0,\n",
    "            \"yanchor\": \"top\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    frames = [{\"data\": frame, \"name\": i} for i, frame in enumerate(data)]\n",
    "    fig.add_traces(frames[0][\"data\"])\n",
    "    fig.update(frames=frames)\n",
    "    fig.update_layout(\n",
    "        updatemenus=updatemenus,\n",
    "        sliders=sliders,\n",
    "        showlegend=False,\n",
    "        scene=dict(\n",
    "            xaxis=dict(showticklabels=False, showgrid=False, zeroline=False, visible=False,),\n",
    "            yaxis=dict(showticklabels=False, showgrid=False, zeroline=False, visible=False,),\n",
    "            zaxis=dict(showticklabels=False, showgrid=False, zeroline=False, visible=False,),\n",
    "        ),\n",
    "        height=1200, width=800\n",
    "    )\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TUM(tum_path, seqlen=20, dilation=19, height=480, width=640)\n",
    "# loader = DataLoader(dataset=dataset, batch_size=1)\n",
    "# colors, depths, intrinsics, poses, *_ = next(iter(loader))\n",
    "\n",
    "# create rgbdimages object\n",
    "# rgbdimages = RGBDImages(colors, depths, intrinsics, poses)\n",
    "\n",
    "# step by step SLAM and store intermediate maps\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "slam = PointFusion(odom='dia', device=device)  # use gt poses because large dilation (small fps) makes ICP difficult\n",
    "pointclouds = Pointclouds(device=device)\n",
    "batch_size, seq_len = rgbdimages.shape[:2]\n",
    "initial_poses = torch.eye(4, device=device).view(1, 1, 4, 4).expand(batch_size, -1, -1, -1)\n",
    "prev_frame = None\n",
    "intermediate_pcs = []\n",
    "poses = torch.empty((1,1,4,4), device=device)\n",
    "for s in range(seq_len):\n",
    "    live_frame = rgbdimages[:, s].to(device)\n",
    "    if s == 0 and live_frame.poses is None:\n",
    "        live_frame.poses = initial_poses\n",
    "    pointclouds, live_frame.poses, live_frame.init_T = slam.step(pointclouds, live_frame, prev_frame)\n",
    "    poses = torch.cat((poses, live_frame.poses), dim=1)\n",
    "    prev_frame = live_frame if slam.odom != 'gt' else None\n",
    "    intermediate_pcs.append(pointclouds[0])\n",
    "\n",
    "# visualize\n",
    "rgbdimages.plotly(0).update_layout(autosize=False, height=600, width=400).show()\n",
    "fig = plotly_map_update_visualization(intermediate_pcs, poses[0], intrinsics[0, 0], 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gradslam')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af38f6cc3291ded760641bf70a2ae3a2429ff6fd74efc4a1fe07b0e8ccb3a34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
